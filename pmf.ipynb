{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Mass Function (PMF)\n",
    "\n",
    "**When to Use**: For discrete variables.\n",
    "\n",
    "**What It Does**: Gives the probability that a discrete random variable is exactly equal to some value.\n",
    "\n",
    "**Important Note**: The sum of all probabilities in a PMF is 1.\n",
    "\n",
    "**Example**: Rolling a die, where the outcome is a discrete number between 1 and 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculating the PMF from a Dataset\n",
    "\n",
    "For discrete random variables, the Probability Mass Function (PMF) can be calculated by counting the occurrences of each value in the dataset and then dividing by the total number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Sample dataset\n",
    "data = np.random.randint(0, 10, size=1000)\n",
    "\n",
    "# Calculate the PMF\n",
    "counts = Counter(data)\n",
    "total_count = sum(counts.values())\n",
    "pmf = {k: v / total_count for k, v in counts.items()}\n",
    "\n",
    "# Plot the PMF\n",
    "plt.bar(pmf.keys(), pmf.values(), label='Estimated PMF')\n",
    "plt.title('PMF Estimation')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Differentiating Between Probability and Probability Mass\n",
    "\n",
    "**Probability**: The likelihood that a given event will occur. For discrete variables, this is straightforward to calculate.\n",
    "\n",
    "Example: The probability of rolling a 3 on a six-sided die is \n",
    "1\n",
    "6\n",
    "6\n",
    "1\n",
    "\n",
    "**Probability Mass Function (PMF)**: For discrete variables, the PMF gives the probability of each possible outcome. It assigns a probability to each distinct value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why It Is Possible to Calculate the Probability for a Specific Point with Discrete Variables\n",
    "\n",
    "For discrete variables, the probability of the variable taking a specific value is non-zero. This is because there are a finite number of possible values the variable can take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estimating Probabilities for Discrete Variables\n",
    "For discrete variables, each specific value has a distinct probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Common Discrete Distributions and Their Use Cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial Distribution:\n",
    "\n",
    "Use Case: Modeling the number of successes in a fixed number of trials.\n",
    "\n",
    "#### Key characteristics of situations that follow a binomial distribution\n",
    "* Fixed number of trials: There's a set number of times the event is repeated (e.g., number of patients in a trial, number of light bulbs tested).\n",
    "* Independent trials: The outcome of one trial doesn't affect the outcome of other trials (e.g., one patient's response to a drug doesn't influence another patient's response).\n",
    "* Two possible outcomes: Each trial has only two possible results (e.g., success/failure, yes/no, make/miss).\n",
    "* Constant probability of success: The probability of success remains the same for each trial (e.g., the probability of a light bulb being defective remains constant throughout the batch).\n",
    "\n",
    "**Real Life Examples**\n",
    "* Medical Trials: When testing new drugs, researchers use binomial distributions to understand how likely it is for a certain number of patients to respond well to the treatment. Each patient in the trial is like a test, where the outcome can be success (the drug works) or failure (it doesn't).\n",
    "\n",
    "* Quality Control: In factories, binomial distributions help ensure product quality. For example, if a factory tests light bulbs and each bulb can either pass (it works) or fail (it's defective), binomial distributions help predict how many defective bulbs might be found in a batch based on a sample.\n",
    "\n",
    "* Survey Responses: Surveys with yes/no questions use binomial distributions to estimate the number of \"yes\" responses. For instance, if a survey asks people if they support a policy change, each person's answer is like a trial with two outcomes: yes or no.\n",
    "\n",
    "* Sports Analytics: Binomial distributions are used in sports to analyze outcomes like free throws in basketball. Each free throw is a trial where the outcome can be success (the shot goes in) or failure (it misses). Analysts use these distributions to predict a player's shooting percentage or the likelihood of a team making a certain number of shots.\n",
    "\n",
    "* Genetics: When studying inheritance of traits, binomial distributions help model the probability of offspring inheriting a specific trait from their parents. Each offspring is like a trial where the outcome can be inheriting the trait or not inheriting it, depending on the genes from each parent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PMF** =P(X=k)=( \n",
    "k\n",
    "n\n",
    "​\n",
    " )p \n",
    "k\n",
    " (1−p) \n",
    "n−k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "n, p = 10, 0.5  # number of trials, probability of success\n",
    "k = np.arange(0, n + 1)\n",
    "binom_pmf = binom.pmf(k, n, p)\n",
    "\n",
    "plt.bar(k, binom_pmf, label='Binomial PMF')\n",
    "plt.title('Binomial Distribution PMF')\n",
    "plt.xlabel('Number of Successes')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Distribution\n",
    "Use Case: Modeling the number of times an event happens in a fixed interval of time or space.\n",
    "\n",
    "**Key Characteristics**\n",
    "\n",
    "* Discrete: It deals with whole numbers of events (you can't have half an event).\n",
    "* Independent Events: The occurrence of one event doesn't affect the probability of another event happening.\n",
    "* Constant Rate: The average rate of events happening is constant over the given time or space.\n",
    "* Rare Events: The probability of an event happening in a short interval is proportional to the length of the interval.\n",
    "\n",
    "**Real-Life Examples**\n",
    "* Number of customer calls to a call center in an hour: If you know the average number of calls the call center receives per hour, you can use the Poisson distribution to predict the probability of receiving a certain number of calls in any given hour. This helps in staffing and resource allocation.\n",
    "\n",
    "* Number of typos on a page of a book: If you have the average number of typos per page in a book, the Poisson distribution can tell you the likelihood of finding a specific number of typos on a single page. This is useful for editors and publishers in quality control.\n",
    "\n",
    "* Number of cars passing through a toll booth in a minute: Knowing the average rate of cars passing through a toll booth per minute allows you to use the Poisson distribution to estimate the probability of observing a particular number of cars passing through in any given minute. This is helpful for traffic management and planning.\n",
    "\n",
    "* Number of mutations in a DNA strand: The Poisson distribution can be applied to model the probability of a specific number of mutations occurring in a DNA strand of a certain length, given the average mutation rate. This is important in genetic research and understanding genetic variability.\n",
    "\n",
    "* Number of earthquakes in a year: If you know the average rate of earthquakes per year in a specific region, the Poisson distribution helps calculate the probability of experiencing a certain number of earthquakes in that region within a year. This aids in seismic hazard assessment and disaster preparedness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PMF** =P(X=k)= \n",
    "k!\n",
    "λ \n",
    "k\n",
    " e \n",
    "−λ\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "λ = 5  # rate (lambda)\n",
    "k = np.arange(0, 20)\n",
    "poisson_pmf = poisson.pmf(k, λ)\n",
    "\n",
    "plt.bar(k, poisson_pmf, label='Poisson PMF')\n",
    "plt.title('Poisson Distribution PMF')\n",
    "plt.xlabel('Number of Events')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Distribution\n",
    "Use Case: Modeling the number of trials until the first success.\n",
    "\n",
    "**Key Characteristics**\n",
    "\n",
    "* Discrete: It deals with whole numbers of trials (you can't have half a trial).\n",
    "* Independent Trials: The outcome of one trial doesn't affect the outcome of other trials.\n",
    "* Two Outcomes: Each trial has only two possible results (success or failure).\n",
    "* Constant Probability of Success: The probability of success remains the same for each trial.\n",
    "* Waiting Time: The random variable is the number of trials needed to get the first success.\n",
    "\n",
    "### Real-Life Examples\n",
    "* Number of coin flips until you get heads: When flipping a fair coin over and over, the number of flips needed to get the first heads follows a geometric distribution. This means each flip is like a trial with two outcomes: heads or tails. The geometric distribution helps predict how many flips it might take on average to get heads for the first time.\n",
    "\n",
    "* Number of raffle tickets bought until you win: If you're buying raffle tickets and each ticket has a chance to win, the number of tickets you need to buy before winning follows a geometric distribution. It helps estimate how many tickets you might need to buy, on average, before winning a prize.\n",
    "\n",
    "* Number of job interviews until you get a job offer: When applying for jobs, the number of interviews you attend before receiving your first job offer can be modeled with a geometric distribution. Each interview is like a trial with two outcomes: getting an offer or not. The distribution helps estimate how many interviews you might go through before getting an offer.\n",
    "\n",
    "* Number of attempts to start a faulty car until it starts: If your car has trouble starting, the number of times you try to start it before it finally starts can follow a geometric distribution. Each attempt is like a trial with two outcomes: the car starts or it doesn't. The distribution helps predict how many attempts you might need, on average, before the car starts.\n",
    "\n",
    "* Number of rolls of a die until you roll a 6: When rolling a die repeatedly, the number of rolls it takes to get a 6 for the first time follows a geometric distribution. Each roll is a trial with six possible outcomes (one for each face of the die). The distribution helps estimate how many rolls, on average, it might take to get a 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PMF** = P(X=k)=(1−p) \n",
    "k−1\n",
    " p \n",
    "\n",
    "where 𝑝\n",
    "* p is the probability of success on each trial.\n",
    "𝑘\n",
    "* k is the number of trials until the first success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import geom\n",
    "\n",
    "p = 0.5  # probability of success\n",
    "k = np.arange(1, 11)\n",
    "geom_pmf = geom.pmf(k, p)\n",
    "\n",
    "plt.bar(k, geom_pmf, label='Geometric PMF')\n",
    "plt.title('Geometric Distribution PMF')\n",
    "plt.xlabel('Number of Trials')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Binomial Distribution\n",
    "Use Case: Modeling the number of trials until a specified number of successes occur.\n",
    "### Key Characteristics\n",
    "\n",
    "* Discrete: It deals with whole numbers of failures (you can't have half a failure).\n",
    "* Independent Trials: The outcome of one trial doesn't affect the outcome of other trials.\n",
    "* Two Outcomes: Each trial has only two possible results (success or failure).\n",
    "* Constant Probability of Success: The probability of success remains the same for each trial.\n",
    "* Fixed Number of Successes: The distribution is defined by the desired number of successes.\n",
    "*Variable Number of Failures: The random variable is the number of failures before achieving the specified number of successes.\n",
    "\n",
    "### Real-Life Examples\n",
    "* Number of misses before making a certain number of free throws: In basketball, if a player has a consistent free throw percentage, the negative binomial distribution can predict how many missed free throws they will have before making a specific number of successful ones. This helps coaches and players understand shooting consistency over multiple attempts.\n",
    "\n",
    "* Number of unsuccessful sales calls before closing a deal: For salespeople, the negative binomial distribution models the number of unsuccessful calls they make before successfully closing a certain number of deals. It helps sales teams estimate the persistence and effort needed to achieve their sales goals.\n",
    "\n",
    "* Number of failed attempts before fixing a bug: In software development, the negative binomial distribution can be used to estimate how many unsuccessful attempts a programmer might make before successfully fixing a bug. It helps in managing time and resources during debugging processes.\n",
    "\n",
    "* Number of lost games before winning a tournament: In sports tournaments, teams may face losses before securing enough wins to advance. The negative binomial distribution helps in predicting how many games a team might lose before achieving a specific number of wins necessary to move forward in the tournament.\n",
    "\n",
    "* Number of unsuccessful attempts before getting a research grant: In academic research, the negative binomial distribution can model the number of unsuccessful grant proposals a researcher submits before receiving funding for a certain number of them. It assists researchers in understanding the likelihood of success based on past experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PMF** = P(X=k)=( \n",
    "r−1\n",
    "k+r−1\n",
    "​\n",
    " )p \n",
    "r\n",
    " (1−p) \n",
    "k\n",
    " \n",
    "\n",
    "* p is the probability of success on each trial.\n",
    "𝑟\n",
    "* r is the number of successes.\n",
    "𝑘\n",
    "* k is the number of failures before achieving \n",
    "𝑟\n",
    "r successes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nbinom\n\u001b[0;32m      3\u001b[0m r, p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# number of successes, probability of success\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m      5\u001b[0m nbinom_pmf \u001b[38;5;241m=\u001b[39m nbinom\u001b[38;5;241m.\u001b[39mpmf(k, r, p)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(k, nbinom_pmf, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNegative Binomial PMF\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import nbinom\n",
    "\n",
    "r, p = 5, 0.5  # number of successes, probability of success\n",
    "k = np.arange(0, 20)\n",
    "nbinom_pmf = nbinom.pmf(k, r, p)\n",
    "\n",
    "plt.bar(k, nbinom_pmf, label='Negative Binomial PMF')\n",
    "plt.title('Negative Binomial Distribution PMF')\n",
    "plt.xlabel('Number of Failures')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
